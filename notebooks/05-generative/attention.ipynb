{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ddmms/camml-tutorials/blob/main/notebooks/05-generative/attention.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the Attention Mechanism\n",
    "\n",
    "Attention is a mechanism that allows models to focus on different parts of an input sequence when producing each output. It was originally developed for sequence-to-sequence models in natural language processing, but it's now used across many domains, including chemistry and materials science.\n",
    "\n",
    "You can think of attention as a **soft lookup**: instead of choosing a single input token to focus on, the model takes a weighted average of all inputs.\n",
    "\n",
    "We’ll start by implementing a simple **scaled dot-product attention** module from scratch using NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention\n",
    "\n",
    "Given:\n",
    "- Query matrix **Q**\n",
    "- Key matrix **K**\n",
    "- Value matrix **V**\n",
    "\n",
    "The attention output is computed as:\n",
    "\n",
    "$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$\n",
    "\n",
    "Where $ d_k $ is the dimension of the key vectors.\n",
    "\n",
    "Intuition:\n",
    "- Compute similarity between each query and all keys\n",
    "- Use these similarities to weight the values\n",
    "\n",
    "Let's implement this using `numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))  # for numerical stability\n",
    "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "# Set dimensions\n",
    "d_k = 4  # dimension of keys/queries\n",
    "seq_len = 3\n",
    "\n",
    "# Random example: 3 tokens with 4-dimensional embeddings\n",
    "np.random.seed(0)\n",
    "Q = np.random.rand(seq_len, d_k)\n",
    "K = np.random.rand(seq_len, d_k)\n",
    "V = np.random.rand(seq_len, d_k)\n",
    "\n",
    "# Attention calculation\n",
    "scores = Q @ K.T / np.sqrt(d_k)     # (3, 3) matrix of attention scores\n",
    "weights = softmax(scores)           # (3, 3) attention weights\n",
    "output = weights @ V                # weighted sum of values\n",
    "\n",
    "print(\"Attention weights:\\n\", weights)\n",
    "print(\"\\nOutput:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding `attention_block`: Scaled Dot-Product Self-Attention in NumPy\n",
    "\n",
    "The `attention_block` function implements **scaled dot-product self-attention** — the core mechanism behind transformer models.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Linear Projections**:  \n",
    "   We project the input $ X $ into three new matrices:\n",
    "   - $ Q = X W_Q $\n",
    "   - $ K = X W_K $\n",
    "   - $ V = X W_V $\n",
    "\n",
    "   Where:\n",
    "   - $ W_Q, W_K, W_V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k} $ are learned projection matrices\n",
    "   - $ d_k $ is typically smaller than $ d_{\\text{model}} $ (but can be equal)\n",
    "\n",
    "2. **Similarity Scores**:  \n",
    "   Compute dot products between all query-key pairs:\n",
    "\n",
    "   $\n",
    "   \\text{scores} = \\frac{Q K^T}{\\sqrt{d_k}}\n",
    "   $\n",
    "\n",
    "   This gives an $ L \\times L $ matrix where each entry represents how much a token should \"attend\" to another token.\n",
    "\n",
    "3. **Softmax**:  \n",
    "   Normalize scores to get **attention weights**:\n",
    "\n",
    "   $\n",
    "   \\alpha_{ij} = \\frac{\\exp(\\text{score}_{ij})}{\\sum_k \\exp(\\text{score}_{ik})}\n",
    "   $\n",
    "\n",
    "   Each row of this matrix sums to 1 — it's a weighted distribution over the input tokens.\n",
    "\n",
    "4. **Weighted Sum of Values**:  \n",
    "   Multiply the attention weights by the value matrix:\n",
    "\n",
    "   $\n",
    "   \\text{Output} = \\alpha V\n",
    "   $\n",
    "\n",
    "   The output is a new representation for each token, where information from the entire sequence has been blended based on relevance.\n",
    "\n",
    "---\n",
    "\n",
    "### Function Signature\n",
    "\n",
    "```python\n",
    "attention_block(X, d_k=None, W_q=None, W_k=None, W_v=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_block(X, d_k=None, W_q=None, W_k=None, W_v=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product self-attention.\n",
    "\n",
    "    Parameters:\n",
    "    - X: np.ndarray of shape (seq_len, d_model) — input embeddings\n",
    "    - d_k: optional int — dimension of Q/K/V projections (default = input dim)\n",
    "    - W_q, W_k, W_v: optional projection matrices of shape (d_model, d_k)\n",
    "\n",
    "    Returns:\n",
    "    - output: np.ndarray of shape (seq_len, d_k)\n",
    "    - weights: attention weight matrix of shape (seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    seq_len, d_model = X.shape\n",
    "    d_k = d_k or d_model  # default to identity if not projecting\n",
    "\n",
    "    # If no projections provided, use random ones\n",
    "    if W_q is None: W_q = np.random.rand(d_model, d_k)\n",
    "    if W_k is None: W_k = np.random.rand(d_model, d_k)\n",
    "    if W_v is None: W_v = np.random.rand(d_model, d_k)\n",
    "\n",
    "    # Project inputs\n",
    "    Q = X @ W_q\n",
    "    K = X @ W_k\n",
    "    V = X @ W_v\n",
    "\n",
    "    # Scaled dot-product attention\n",
    "    scores = Q @ K.T / np.sqrt(d_k)        # Shape: (seq_len, seq_len)\n",
    "    weights = softmax(scores)              # Shape: (seq_len, seq_len)\n",
    "    output = weights @ V                   # Shape: (seq_len, d_k)\n",
    "\n",
    "    return output, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run it on dummy data\n",
    "X = np.random.rand(3, 8)  # 3 tokens with 8-dim embeddings\n",
    "out, attn_weights = attention_block(X, d_k=4)\n",
    "\n",
    "print(\"Attention Output:\\n\", out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the results\n",
    "\n",
    "We can visualise the attention matrix, which controls how much the tokens affect one another. Remember that at this stage the attention matrix is random. During training the model would learn the weights of this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_attention(weights, title=\"Attention Weights\"):\n",
    "    plt.imshow(weights, cmap='turbo')\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Key Index\")\n",
    "    plt.ylabel(\"Query Index\")\n",
    "    plt.show()\n",
    "\n",
    "plot_attention(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention with words\n",
    "\n",
    "Let's make that a little bit more concrete by implementing attention for a sentence. In this case we have a simple 6 word sentence with 5 different words. We will use a rudimentary tokenizing scheme whereby each word is allocated a number, corresponding to where it first occurs in the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Tokenize the sentence\n",
    "sentence = \"The cat sat on the mat\"\n",
    "tokens = sentence.lower().split()\n",
    "vocab = list(set(tokens))  # Unique tokens\n",
    "vocab_size = len(vocab)\n",
    "token_to_id = {tok: i for i, tok in enumerate(vocab)}\n",
    "\n",
    "# Map sentence to token IDs\n",
    "token_ids = np.array([token_to_id[tok] for tok in tokens])\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the embedding matrix\n",
    "\n",
    "The embedding matrix will determine the dimensionality of the tokens within the attention block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create random embeddings\n",
    "embedding_dim = 8\n",
    "np.random.seed(3)\n",
    "embedding_matrix = np.random.rand(vocab_size, embedding_dim)\n",
    "\n",
    "# Get the embedded sentence\n",
    "X = embedding_matrix[token_ids]  # Shape: (seq_len, embedding_dim)\n",
    "print(\"Embedded shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, attn_weights = attention_block(X, d_k=embedding_dim)\n",
    "X_out = X + out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the attention matrix\n",
    "\n",
    "We can visualise the attention matrix, which controls how much the tokens affect one another. Remember that at this stage the attention matrix is random. During training the model would learn the weights of this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_tokens(weights, tokens):\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.imshow(weights, cmap='Oranges')\n",
    "    plt.xticks(ticks=np.arange(len(tokens)), labels=tokens, rotation=45)\n",
    "    plt.yticks(ticks=np.arange(len(tokens)), labels=tokens)\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Self-Attention Weights\")\n",
    "    plt.xlabel(\"Key Tokens\")\n",
    "    plt.ylabel(\"Query Tokens\")\n",
    "    plt.show()\n",
    "\n",
    "plot_attention_tokens(attn_weights, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the effect of attention\n",
    "\n",
    "The attention matrix tells us how one token in a sequence updates another. In order to visualise this we can plot the token vectors before and after passing through the attention block. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_input_output(X, out, tokens=None, title=\"Input vs Attention Output\"):\n",
    "    \"\"\"\n",
    "    Plots the input and output embeddings side by side as heatmaps.\n",
    "\n",
    "    Parameters:\n",
    "    - X: input array (seq_len, dim)\n",
    "    - out: output from attention (seq_len, dim)\n",
    "    - tokens: optional list of token labels for y-axis\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Plot input embeddings\n",
    "    im0 = axs[0].imshow(X, aspect='auto', cmap='Blues')\n",
    "    axs[0].set_title(\"Input Embeddings\")\n",
    "    axs[0].set_xlabel(\"Embedding Dim\")\n",
    "    axs[0].set_ylabel(\"Token\")\n",
    "    if tokens:\n",
    "        axs[0].set_yticks(range(len(tokens)))\n",
    "        axs[0].set_yticklabels(tokens)\n",
    "    fig.colorbar(im0, ax=axs[0])\n",
    "\n",
    "    # Plot attention output\n",
    "    im1 = axs[1].imshow(out, aspect='auto', cmap='Blues')\n",
    "    axs[1].set_title(\"Output Embeddings (after Attention)\")\n",
    "    axs[1].set_xlabel(\"Embedding Dim\")\n",
    "    axs[1].set_ylabel(\"Token\")\n",
    "    if tokens:\n",
    "        axs[1].set_yticks(range(len(tokens)))\n",
    "        axs[1].set_yticklabels(tokens)\n",
    "    fig.colorbar(im1, ax=axs[1])\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_input_output(X, X_out, tokens=tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is Attention Powerful?\n",
    "\n",
    "- It lets the model decide **what to look at** in the input when processing each token.\n",
    "- It allows **long-range dependencies** (e.g. token 1 can directly interact with token 50).\n",
    "- It’s **parallelizable** across all positions (unlike RNNs).\n",
    "\n",
    "This is the foundation of the **Transformer architecture**, where multiple attention heads are used in parallel.\n",
    "\n",
    "In later notebooks, we’ll see how this mechanism is used in CrystaLLM for crystal generation and materials modeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alignn-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
