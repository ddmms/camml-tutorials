{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31d09edf-3aa7-4fc3-aa70-f16d432fa19b",
   "metadata": {},
   "source": [
    "# Convolutional Graph Networks\n",
    "\n",
    "In this notebook, we will construct a convolutional graph neural network and use it predict the elastic properties of materials. We implement the [CGCNN model](https://github.com/txie-93/cgcnn) from scratch using pytorch.\n",
    "\n",
    "The notebook is split into three sections:\n",
    "1. Data loading and analysis\n",
    "2. Model construction\n",
    "3. Model training and evaluation\n",
    "\n",
    "The notebook depends on:\n",
    "- torch\n",
    "- ase\n",
    "- torch-scatter\n",
    "\n",
    "Alex Ganose, *Imperial College London* (2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad3b4a3-30b3-4242-95b5-5cf62aa46a3f",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "First, install the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d926d8b2-6c38-441f-9680-23418d27e1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch_scatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18488800-caae-43d1-a42a-1449ea7504ab",
   "metadata": {},
   "source": [
    "Now, import all the functions and modules necessary to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcac8879-32ea-4112-934a-3735e73ee887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_scatter\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from utils import MaterialsDataset, collate_fn, plot_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d199487-12f5-44e5-b452-6b2ac4b3601a",
   "metadata": {},
   "source": [
    "## Ensure reproducibility\n",
    "\n",
    "We need to set a random seed to ensure consistent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f5103e-a210-4588-b0d5-54f255c5eb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38318f33-f64b-46c2-8c5a-637bff568732",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "We will be using the [matbench](http://matbench.materialsproject.org) `log_gvrh` dataset which contains 10,987 VRH-average shear moduli calculated using DFT.\n",
    "The dataset has been preprocessed by taking the base 10 logarithm as the data spans several order of magnitude.\n",
    "\n",
    "The original dataset can be obtained using the [`matminer`](https://hackingmaterials.lbl.gov/matminer/) package, however for this notebook, we have included a version of the dataset with some adjustments to the data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5404ce-7c4b-4933-806f-13a9c470859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MaterialsDataset(\n",
    "    \"bulk-modulus-dataset.json\",\n",
    "    cutoff=4,  # cutoff radius for finding neighbours\n",
    "    num_gaussians=40,  # number of gaussians in edge embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937051c7-7724-4fc9-9662-9734e46337f9",
   "metadata": {},
   "source": [
    "## Take a look at the data\n",
    "\n",
    "It's always a good idea to look a bit at the data. First, let's plot a histogram of the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bc47f6-fac6-4feb-99bb-36646e97a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [x.target[0] for x in dataset]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(values, bins=50)\n",
    "ax.set(xlabel=\"log10(G$_\\mathrm{VRH}$)\", ylabel=\"Number of examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01df972d-ef43-4043-bc80-4fef4f37c81d",
   "metadata": {},
   "source": [
    "Now let's look at the structure of the data. Each sample has the following information:\n",
    "\n",
    "- `node_feat`: The node features as a ($N$, num_node_features) tensor.\n",
    "- `edge_feat`: The edge features as a ($M$, num_edge_features) tensor.\n",
    "- `edge_src` : The index of the central node for each edge.\n",
    "- `edge_dst` : The index of the destination node for each edge.\n",
    "- `target` : The target property to learn.\n",
    "- `atoms`: An ase atoms object, representing the structure.\n",
    "\n",
    "Here, $N$ and $M$ are the number of nodes and edges in the structure, respectively.\n",
    "\n",
    "Let's have a look at the node features for the first material in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa6f34b-5537-450d-8e97-93ddb459b8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[0]\n",
    "sample.node_feat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d536d6-ed92-47b8-b372-780e62dc2468",
   "metadata": {},
   "source": [
    "This tells us there are 5 atoms in the structure, and each node has 119 features. The initial node features are just a one-hot encoding of the atom type. If we look at the first atom, we can see it has a `1` at the 19th index in the tensor and zeros elsewhere, indicating that this is the 20th element in the periodic table, i.e., calcium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b926f40e-a198-42c6-8ae2-89d574c9a4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample.node_feat[0])\n",
    "print(sample.node_feat[0][19])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff10833-920c-4e4b-9341-966ec49d1b44",
   "metadata": {},
   "source": [
    "Let's also look at the edge features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dcd4f0-a7bf-4d3b-a9c8-4e8376b64d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.edge_feat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8916a5fe-9812-4526-b652-18c4ed936ec8",
   "metadata": {},
   "source": [
    "Here we can see there are 58 edges in the structure. On average, that works out to ~10 edges per atom. There are 40 features per edge. Let's have a look at the features for the first edge in the material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1162182f-84af-4dcb-999a-cbd507235d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.edge_feat[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e03dd14-305f-45d4-b092-755f88173c92",
   "metadata": {},
   "source": [
    "The edge embedding is the bond length projected onto a basis of 40 equally spaced gaussians. To understand this visually, consider the following plot where was have drawn a schematic of the gaussian basis functions. Each coloured line corresponds to one basis function. In this case we are only plotting 10 basis functions for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2042654-d013-4711-8e8a-76ac04741a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "dists = np.linspace(0, 4, 100)\n",
    "centres = np.linspace(0, 4, 10)\n",
    "edge_feat = np.exp(-((dists[:, None] - centres) ** 2) / (centres[1] - centres[0]) ** 2)\n",
    "ax.plot(dists, edge_feat, label=[f\"Basis {i+1}\" for i in range(10)])\n",
    "ax.axvline(2.5, ls=\"--\", color=\"k\")\n",
    "ax.set(xlabel=\"Distance (Å)\", ylabel=\"Feature value\")\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc=\"upper left\", frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3952f8-4ab8-42c2-b581-a9b087fed5ea",
   "metadata": {},
   "source": [
    "To understand the embedding, consider a bond length of 2.5 Å shown by the black dotted line. For each basis function, the value of the function at 2.5 Å is used to form the embedding. IN this case, the first 3 basis functions have a value very close to zero at 2.5 Å, whereas basis functions 6 and 7 will have high values.\n",
    "\n",
    "In this example, the embedding would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b47a30-907f-4e04-9009-6b0c66786617",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_feat = np.exp(-((2.5 - centres) ** 2) / (centres[1] - centres[0]) ** 2).round(3)\n",
    "edge_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8268e0-2d30-4c27-aa63-92b2f218ed66",
   "metadata": {},
   "source": [
    "The final piece of information necessary for convolutional graph networks are the indices of the central and destination nodes for each bond. Let's look at the source indices for our sample structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8211042-7259-41b2-8c50-153990dd87f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.edge_src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7bfc1b-a663-466e-9b5f-aedf18045aa8",
   "metadata": {},
   "source": [
    "The numbers correspond to the index of the atoms in the structure. In our structure there are 5 atoms, so the indices only go up to 4 (all python arrays are zero-indexed).\n",
    "The first 15 numbers are all zero, indicating that the first atom in the structure has 15 bonds.\n",
    "\n",
    "Now let's look at the destination indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da9dba9-0ebd-4213-919f-bc812d960770",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.edge_dst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273fa207-0664-4efc-bc1d-4bdd74b92783",
   "metadata": {},
   "source": [
    "Concentrating on the first 15 numbers (corresponding to atoms bonded to the first atom in the structure) you can see that there can be more than one bond to the same destination atom. The reason for this is periodic boundary conditions. Some of these bonds will be to atoms in adjacent unit cells. For that reason, their bond lengths and corresponding edge embeddings will all be different.\n",
    "\n",
    "You can think of the `edge_src` and the `edge_dst` as the node $i$ and $j$ indices from the lecture slides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a244e6cf-4290-41ea-a4d7-a785f9d5d556",
   "metadata": {},
   "source": [
    "We can visualise the structure and corresponding graph using the builtin helper function. Note, the plot only shows one edge per pair of atoms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32697a6-dbf5-48fc-bd5c-a7e9e8947b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65e0244-bd75-45c9-b635-a0fa1ebee6bf",
   "metadata": {},
   "source": [
    "## Split the data for train, test, and validation\n",
    "\n",
    "Now we split the dataset into the training, testing, and validation sets. Recall that the validation set is different from the test set.\n",
    "The validation set can be used to select hyperparameters, so is strictly part of the model selection process.\n",
    "\n",
    "We use 80% of the data for training, 10% for validation, and 10% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dcdb1d-cad6-4183-8cbe-b11fcc12ac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set, test_set = random_split(dataset, [0.8, 0.1, 0.1])\n",
    "\n",
    "print(f'Number of training examples: {len(train_set)}')\n",
    "print(f'Number of validation examples: {len(valid_set)}')\n",
    "print(f'Number of testing examples: {len(test_set)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d542d5-cb03-451c-aaa4-8e5a4c762eb3",
   "metadata": {},
   "source": [
    "## Set up the batches\n",
    "\n",
    "We can also set up the mini-batches for training. Here, we use a custom \"collate function\" that will automatically handle grouping multiple graphs into a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9840dfe1-fbe2-4050-8607-a9f93e3c158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True,\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_set, batch_size=BATCH_SIZE, collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_set, batch_size=BATCH_SIZE, collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdb9a03-7ca4-4359-9b5a-4818af44cb15",
   "metadata": {},
   "source": [
    "## Explore the batch data format\n",
    "\n",
    "Before starting to construct the neural network architecture, it is worth spending a little time getting familiar with the format of the batched data.\n",
    "\n",
    "Each batch has the following information:\n",
    "\n",
    "- `node_feat`: The node features as a ($N$, n_node_feats) tensor.\n",
    "- `edge_feat`: The edge features as a ($M$, n_edge_feats) tensor.\n",
    "- `edge_src` : The index of the central node for each edge.\n",
    "- `edge_dst` : The index of the destination node for each edge.\n",
    "- `target` : The target property to learn, as a ($G$, 1) tensor.\n",
    "- `batch` : The graph to which each node belongs, as a ($N$, ) Tensor.\n",
    "\n",
    "Here $N$ and $M$ are the total number of atoms *in the batch*. $G$ is the total number of graphs (structures) in the batch.\n",
    "\n",
    "Let's have a look at the node features for the first batch of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3399a116-17a7-4a80-a087-798aa6aeeb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(test_loader))\n",
    "batch.node_feat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1db241-93bd-444b-a345-619377fe3629",
   "metadata": {},
   "source": [
    "Notice how now there are many more nodes (518 of them). The number of node features (i.e., the one-hot enconding of atom types) has remained the same.\n",
    "\n",
    "The number of edges in the batch has also increased significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e55c841-1373-4d27-9011-301e0fe8ef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.edge_feat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1436a37-9442-43af-8182-b9f193614a2c",
   "metadata": {},
   "source": [
    "Because we need to handle multiple graphs simultaneously, the `batch.batch` attribute is essential to know which graph each atom belongs to.\n",
    "Let's look at the first 100 values in this array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4affde-5ab2-43a1-96fb-cf0deb426e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.batch[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3453edb7-b8d1-4ffd-b34d-5a6ebaff0c4e",
   "metadata": {},
   "source": [
    "This tells us that the first five nodes in the `batch.node_feat` array correspond to the first structure, while the next two nodes belong to the second structure, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22ce99c-5742-4d20-89eb-c336e72d6a7a",
   "metadata": {},
   "source": [
    "## Crystal Graph Convolutional Neural Networks – Convolution\n",
    "\n",
    "The first step in building the model is defining a graph convolution.\n",
    "\n",
    "In the CGCNN model, we need to compute the following steps:\n",
    "\n",
    "$$\n",
    "\\mathbf{m}_i^{(t)} = \\mathbf{v}_i^{(t)} \\bigoplus \\mathbf{v}_j^{(t)} \\bigoplus \\mathbf{e}_{ij}^{(t)}\n",
    "$$\n",
    "\n",
    "where $i$ and $j$ are the indices of the central and neighbouring atoms, $\\mathbf{v}^{(t)}$ and $\\mathbf{e}^{(t)}$ are the node and edge embeddings for convolution $t$. The update function is performed as:\n",
    "\n",
    "$$\n",
    "\\mathbf{z}_i^{(t)} = \\mathbf{v}_i^{(t)} + \\sum_{j \\in \\mathcal{N}(i)}\n",
    " \\sigma \\left ( \\mathbf{W}_f^{(t)} \\mathbf{m}_i^{(t)} + \\mathbf{b}_f^{(t)} \\right )\n",
    " \\bigodot g \\left ( \\mathbf{W}_s^{(t)} \\mathbf{m}_i^{(t)} + \\mathbf{b}_s^{(t)} \\right )\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_i^{(t+1)} = g(\\mathbf{z}_i^{(t)})\n",
    "$$\n",
    "\n",
    "where $\\sigma$ and g are the sigmoid and softplus activation functions, and $\\mathbf{W}$ and $\\mathbf{b}$ are the weights and biases. \n",
    "\n",
    "## Convolution implementation\n",
    "\n",
    "As discussed in the previous lesson, in pytorch we build networks as a class. Below, we have implemented the convolution layer from CGCNN. \n",
    "The steps required to build a network can be summarised as:\n",
    "- Declare the class - it should be a subclass of the `nn.Module` class from `pytorch`\n",
    "- Define what inputs it takes upon declaration - in this case `node_feat_dim` and `edge_feat_dim`\n",
    "- `super` makes sure it inherits attributes from nn.Module\n",
    "- We then define the different types of layers that we will use – in this case we have a linear layer and two batch normalisation layers.\n",
    "- Then we define a method `forward` which is what gets called when data is passed through the network, this basically moves the data x through the layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080fc813-8a82-4a08-ae87-39ab2def725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    \n",
    "    def __init__(self, node_feat_dim, edge_feat_dim):\n",
    "        \"\"\"\n",
    "        Convolutional layer for graphs.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        node_feat_dim : int\n",
    "          Number of node features.\n",
    "        edge_feat_dim : int\n",
    "          Number of edge features.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # linear layer used for the gated MLP\n",
    "        self.lin1 = nn.Linear(\n",
    "            2 * node_feat_dim + edge_feat_dim,\n",
    "            2 * node_feat_dim,\n",
    "        )\n",
    "\n",
    "        # normalisation layers\n",
    "        self.bn1 = nn.BatchNorm1d(2 * node_feat_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(node_feat_dim)\n",
    "\n",
    "    def forward(self, node_feat, edge_feat, edge_src, edge_dst):\n",
    "        \"\"\"Perform the convolution.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node_feat : Tensor\n",
    "            The node features.\n",
    "        edge_feat : Tensor\n",
    "            The edge features\n",
    "        edge_src : Tensor\n",
    "            The indices of the central nodes for the edges.\n",
    "        edge_dst : Tensor\n",
    "            The indices of the desintation nodes for the edges.\n",
    "        \"\"\"\n",
    "        # concatenate node and edge features\n",
    "        m = torch.cat([node_feat[edge_src], node_feat[edge_dst], edge_feat], dim=1)\n",
    "\n",
    "        # gated MLP\n",
    "        z = self.lin1(m)\n",
    "        z = self.bn1(z)\n",
    "        z1, z2 = z.chunk(2, dim=1)\n",
    "        z1 = nn.Sigmoid()(z1)\n",
    "        z2 = nn.Softplus()(z2)\n",
    "        z = z1 * z2\n",
    "\n",
    "        # pool features\n",
    "        z = torch_scatter.scatter_add(z, edge_src, out=torch.zeros_like(node_feat), dim=0)\n",
    "        \n",
    "        # pass through normalisation layer\n",
    "        return nn.Softplus()(self.bn2(z) + node_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a789075b-1df1-4666-9a6d-2a921ab82c13",
   "metadata": {},
   "source": [
    "Have a look at the `forward` method of the convolution and see if you can understand the components and their relation to the formulas described above. Note that the implementation includes two normalisation layers `bn1` and `bn2` that are omitted from the formulas for clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050554ea-c75f-4fa1-b5b9-857cd82c5a88",
   "metadata": {},
   "source": [
    "## Full graph network architecture\n",
    "\n",
    "Now we have the convolution, we can construct the full CGCNN model.\n",
    "\n",
    "The additional components of the CGCNN architecture are as follows:\n",
    "- An MLP that transforms the one-hot encoded node features to the node embedding. The dimensionality of the node embedding is controlled by `node_hidden_dim`.\n",
    "- The pooling of node features to obtain the crystal features $\\mathbf{u}_c$, calculated as:\n",
    "\n",
    "$$\n",
    "\\mathbf{u}_c = \\sum_{i \\in \\mathcal{G}} \\frac{\\mathbf{v}_i^{(T)}}{|\\mathcal{G}|}\n",
    "$$\n",
    "- An MLP that takes $\\mathbf{u}_c$ and outputs the target property. In CGCNN this consists of a fully-connected layer with soft-plus activation, followed by a fully-connected layer with no-activation function.\n",
    "\n",
    "Below we give an implementation of the CGCNN model. The number of convolutions can be controlled using the `num_graph_conv_layers` parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca0add4-d8b2-4c2c-9173-4f556e614438",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_feat_dim,\n",
    "        edge_feat_dim,\n",
    "        node_hidden_dim=64,\n",
    "        num_graph_conv_layers=3,\n",
    "        fc_feat_dim=128\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Crystal Graph Convolutional Neural Network \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node_feat_dim : int\n",
    "          Number of initial node features from one-hot encoding.\n",
    "        edge_feat_dim : int\n",
    "          Number of bond features.\n",
    "        node_hidden_dim : int\n",
    "          The number of features in the node embedding.\n",
    "        num_graph_conv_layers: int\n",
    "          Number of convolutional layers.\n",
    "        fc_feat_dim: int\n",
    "          Number of hidden features after pooling.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # dense layer to transform one-hot encoded node features to embedding\n",
    "        self.embedding = nn.Linear(node_feat_dim, node_hidden_dim)\n",
    "\n",
    "        # set up the convolutions\n",
    "        convs = []\n",
    "        for _ in range(num_graph_conv_layers):\n",
    "            convs.append(GraphConvolution(node_feat_dim=node_hidden_dim, edge_feat_dim=edge_feat_dim))\n",
    "        self.convs = nn.ModuleList(convs)\n",
    "\n",
    "        # dense layer to turn final node embeddings to the crystal features\n",
    "        self.conv_to_fc = nn.Sequential(\n",
    "           nn.Linear(node_hidden_dim, fc_feat_dim), nn.Softplus()\n",
    "        )\n",
    "\n",
    "        # dense layer to get the final target value\n",
    "        self.fc_out = nn.Linear(fc_feat_dim, 1)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "        Predict the target property given a batch of data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch : Batch\n",
    "            The data to pass through the network.\n",
    "        \"\"\"\n",
    "        # get initial node embedding\n",
    "        node_feat = self.embedding(batch.node_feat)\n",
    "\n",
    "        # apply convolutions\n",
    "        for conv_func in self.convs:\n",
    "            node_feat = conv_func(node_feat, batch.edge_feat, batch.edge_src, batch.edge_dst)\n",
    "\n",
    "        # pool node vectors\n",
    "        crys_feat = torch_scatter.scatter_mean(node_feat, batch.batch, dim=0)\n",
    "\n",
    "        # pass pooled vector through FC layer with activation\n",
    "        crys_feat = self.conv_to_fc(crys_feat)\n",
    "\n",
    "        # pass crystal features through final fully-connected layer\n",
    "        return self.fc_out(crys_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3369fde-5a00-43be-8180-e9c0e76305b0",
   "metadata": {},
   "source": [
    "Look at the `forward` method and see if you can understand the components and their relation to the formulas given above.\n",
    "\n",
    "We can now use the `CGCNN` class to build the full network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d44d8e0-d51a-4fba-90bf-04bf8477c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_NODE_FEATURES = sample.node_feat.size()[1]\n",
    "NUM_EDGE_FEATURES = sample.edge_feat.size()[1]\n",
    "\n",
    "NUM_CONVOLUTIONS = 3\n",
    "\n",
    "model = CGCNN(\n",
    "    NUM_NODE_FEATURES,\n",
    "    NUM_EDGE_FEATURES,\n",
    "    num_graph_conv_layers=NUM_CONVOLUTIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad258e1e-beea-4e42-834e-20ff7ad2ffd0",
   "metadata": {},
   "source": [
    "##  Training the Model\n",
    "\n",
    "Next, we'll define our optimizer. This is the algorithm we will use to update the parameters of our model with respect to the loss calculated on the data. We aren't going to go into too much detail on how neural networks are trained (see [this](http://neuralnetworksanddeeplearning.com/) article if you want to know how) but the gist is:\n",
    "- pass a batch of data through your model\n",
    "- calculate the loss of your batch by comparing your model's predictions against the target value\n",
    "- calculate the gradient of each of your parameters with respect to the loss\n",
    "- update each of your parameters by subtracting their gradient multiplied by a small learning rate parameter\n",
    "\n",
    "We use the AdamW optimiser with the default parameters to update our model. Improved results could be obtained by searching over different optimizers and learning rates, however AdamW is a reasonable starting off point. Check out [this](https://ruder.io/optimizing-gradient-descent/) article if you want to learn more about the different optimization algorithms commonly used for neural networks.\n",
    "\n",
    "Then, we define a criterion, PyTorch's name for a loss/cost/error function. This function will take in your model's predictions with the actual target value and then compute the loss/cost/error of your model with its current parameters.\n",
    "\n",
    "In this case, we will use the mean squared error loss function, defined as:\n",
    "\n",
    "$$\n",
    "\\mathrm{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "where $n$ is the number of data points, $y_i$ are the actual values, and $\\hat{y}_i$ are the predicted values from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca2bc35-8ba9-46e3-8400-ac4a9b54d785",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d992cb3c-d4b2-449f-9680-07c7acf486bd",
   "metadata": {},
   "source": [
    "## Look for GPUs\n",
    "In pytorch automatically defaults to run on cpu. You can check for avialible gpus, then move all of the code across to GPU if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63283cf5-eb9c-494c-bd36-1a6d918a9ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd4f694-b0d7-4c04-989d-83e257065e20",
   "metadata": {},
   "source": [
    "## Define a training loop\n",
    "\n",
    "We now define a function to train the model. This function will:\n",
    "- put our model into train mode\n",
    "- iterate over our dataloader, returning one batch at a time\n",
    "- place the batch on to our GPU, if we have one\n",
    "- clear the gradients calculated from the last batch\n",
    "- pass our batch through the CGCNN model to get out predictions, `y_pred`\n",
    "- calculate the loss between our predictions and the actual target value\n",
    "- calculate the gradients of each parameter\n",
    "- update the parameters by taking an optimizer step\n",
    "- update our metrics\n",
    "\n",
    "Some layers act differently when training and evaluating the model that contains them, hence why we must tell our model we are in \"training\" mode. The model we are using here does not use any of those layers, however it is good practice to get used to putting your model in training mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f89309a-2867-467c-ae66-f05543c87cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    epoch_loss = 0\n",
    "    epoch_mae = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # move the data onto the GPU if available\n",
    "        batch.to(device)  \n",
    "        \n",
    "        # compute output\n",
    "        y_pred = model(batch)\n",
    "        loss = criterion(y_pred, batch.target)\n",
    "        mae = nn.L1Loss()(y_pred, batch.target)\n",
    "        \n",
    "        # compute gradient\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update metrics\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_mae += mae.item()\n",
    "        \n",
    "    return epoch_loss / len(dataloader), epoch_mae / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645cafef-d947-4cd4-9787-2f9ab6fc0366",
   "metadata": {},
   "source": [
    "# Set up an evaluation loop\n",
    "\n",
    "This is very similar to the training loop, except that we do not pass the gradients back to updated the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b2e934-4267-43f2-af4a-6465ff5c8571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    epoch_loss = 0\n",
    "    epoch_mae = 0\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            batch.to(device)  \n",
    "            \n",
    "            # compute output\n",
    "            y_pred = model(batch)\n",
    "            loss = criterion(y_pred, batch.target)\n",
    "            mae = nn.L1Loss()(y_pred, batch.target)\n",
    "                   \n",
    "            # update metrics\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_mae += mae.item()\n",
    "        \n",
    "    return epoch_loss / len(dataloader), epoch_mae / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc30dac4-b411-4096-916d-d9e661008e6b",
   "metadata": {},
   "source": [
    "## Run the training\n",
    "\n",
    "Here we will train for 30 epochs. At the end of each epoch we check the validation loss, if it is better than the previous best, then we save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc23c16-8811-4c09-9a47-12db5efb958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 40\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "history = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    train_loss, train_mae = train(model, train_loader, criterion, optimizer, device)\n",
    "    valid_loss, valid_mae = evaluate(model, valid_loader, criterion, device)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'elastic-model.pt')\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    history.append({'epoch': epoch, 'epoch_time': epoch_time, \n",
    "                    'valid_mae': valid_mae, 'train_mae': train_mae})\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Train MAE: {train_mae:8.4f} | Valid MAE: {valid_mae:8.4f}'\n",
    "         f' |   Elapsed time: {time.strftime(\"%M min %S s\", time.gmtime(epoch_time))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edbbe07-685a-4fec-a8dd-93ad277f2525",
   "metadata": {},
   "source": [
    "## Plotting model training\n",
    "\n",
    "We can plot the performance of the model during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3cc0e1-f070-4630-aebd-f1e27fbad29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [x[\"epoch\"] for x in history]\n",
    "train_loss = [x[\"train_mae\"] for x in history]\n",
    "valid_loss = [x[\"valid_mae\"] for x in history]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epochs, train_loss, label=\"train\")\n",
    "ax.plot(epochs, valid_loss, label=\"valid\")\n",
    "ax.set(xlabel=\"Epoch\", ylabel=\"MAE\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6416b16-870f-4b5a-afe4-fdf195e37bc9",
   "metadata": {},
   "source": [
    "## Evaluating model accuracy on the test set\n",
    "\n",
    "Now we can evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fddfef1-62c1-4f3a-86f3-381421d1188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('elastic-model.pt'))\n",
    "test_loss, test_mae = evaluate(model, test_loader, criterion, device)\n",
    "print(f\"Test MAE: {test_mae:8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72762799-28c3-4931-96b6-9b6db0e6df69",
   "metadata": {},
   "source": [
    "We can also plot the predictions of the model against the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd5ce90-44e8-4457-b833-fbfab8498a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "actual = []\n",
    "predicted = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        batch.to(device)  \n",
    "        y_pred = model(batch)\n",
    "        predicted.extend(y_pred.data.cpu().tolist())\n",
    "        actual.extend(batch.target.tolist())\n",
    "        \n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(actual, predicted, label=\"train\")\n",
    "ax.plot([0, 3], [0, 3], ls=\"--\", c=\"grey\")\n",
    "ax.set(ylabel=\"Predicted log10(G$_{vrh}$)\", xlabel=\"Actual log10(G$_{vrh}$)\", xlim=(0, 3), ylim=(0, 3))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93091f5a-f1da-48fb-bb18-f24eb950de9a",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Assess the impact of changing the number of convolutions on the model accuracy. What is the optimum value for this dataset.\n",
    "2. Try changing the node embedding length. Is the default value optimal.\n",
    "3. How does the performance of our model compare to the state-of-the-art on the [matbench website](https://matbench.materialsproject.org)?\n",
    "4. Advanced exercise: Currently our CGCNN model only has a single fully-connected before the final read-out step. The performance of the model could be improved by adding multiple dense layers before the read out. Try adding these extra layers with softplus activation and see how they impact the model performance.\n",
    "\n",
    "   *Hint:* the dense layers should be added just before the `return self.fc_out(crys_feat)` line in the `CGCNN.forward` function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
